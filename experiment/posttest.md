#### Please attempt the following questions

<br>
Q1. The single-layer perceptron can only learn linearly separable patterns because:<br>
a. It uses a linear activation function <br>
b. It lacks hidden layers for complex feature representations <br>
c. It cannot handle non-linear relationships <br>
d. Both b) and c) <br>
<br>

Q2. The single-layer perceptron algorithm can converge to a solution if:<br>
a. The data is linearly separable <br>
b. The data is linearly non-separable <br>
c. The learning rate is too high <br>
d. The activation function is non-linear <br>
<br>

Q3. The decision boundary of a single-layer perceptron is:<br>
a. Always linear <br>
b. Always non-linear <br>
c. Linear for linearly separable problems, non-linear otherwise <br>
d. Non-linear for linearly separable problems, linear otherwise <br>
<br>

Q4. How many output nodes are there in a Single Layer Perceptron designed for binary classification?<br>
a. One <br>
b. Two <br>
c. Depends on the input size <br>
d. Variable <br>
<br>

Q5. The perceptron learning rule updates the weights based on:<br>
a. The difference between the predicted output and the target output <br>
b. The derivative of the activation function <br>
c. The gradient of the cost function <br>
d. The maximum likelihood estimation <br>
<br>



